{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8641570,"sourceType":"datasetVersion","datasetId":5175399},{"sourceId":8648587,"sourceType":"datasetVersion","datasetId":5180305},{"sourceId":8649137,"sourceType":"datasetVersion","datasetId":5180700}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport gym\nimport numpy as np\nimport random\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nfrom sklearn.preprocessing import StandardScaler\nimport random\n# Normalize features\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import deque\n\nimport collections\nfrom keras import backend as K\nimport tensorflow as tf\nfrom gym import spaces\nimport matplotlib.pyplot as plt\nK.clear_session()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:28:59.456065Z","iopub.execute_input":"2024-06-09T20:28:59.456928Z","iopub.status.idle":"2024-06-09T20:29:15.912010Z","shell.execute_reply.started":"2024-06-09T20:28:59.456888Z","shell.execute_reply":"2024-06-09T20:29:15.910973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/eqqerff/preprocessed.csv')\ndata.drop(columns= ['Unnamed: 0','index'], inplace = True)\n# preprocessing\ndata['DATETIME'] = pd.to_datetime(data['DATETIME'])\ndata['TIME'] = data['TIME'].astype(str).str.rstrip('000')\ndata['hour'] = data['DATETIME'].dt.hour\ndata['minute'] = data['DATETIME'].dt.minute\ndata['day_of_week'] = data['DATETIME'].dt.day_name()\ndata['second'] =  data['DATETIME'].dt.second\nday_map = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\ndata['day_of_week'] = data['day_of_week'].map(day_map)\ndata.drop(columns= ['DATETIME','TIME'], inplace = True) \ndata.drop(columns= ['TRADE_PRICE','TRADENO'], inplace = True)\ndata= data.dropna()\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:29:15.914126Z","iopub.execute_input":"2024-06-09T20:29:15.914704Z","iopub.status.idle":"2024-06-09T20:29:20.233762Z","shell.execute_reply.started":"2024-06-09T20:29:15.914675Z","shell.execute_reply":"2024-06-09T20:29:20.232743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reward_function(df):\n    # Calculate best bid-ask spread for each date\n    best_spread_per_date = df.groupby('DATE').apply(lambda x: x['BID_PRICE'].max() - x['ASK_PRICE'].min())\n\n    rewards = []\n    for index, row in df.iterrows():\n        # Calculate execution price deviation from best bid-ask spread\n        execution_price_deviation = row['PRICE'] - best_spread_per_date[row['DATE']]\n\n        # Calculate execution speed penalty\n        execution_speed_penalty = 100 / 1000  # converting milliseconds to seconds\n\n        # Calculate commission cost\n        commission_cost = 0.01 * row['TRADE_VOLUME']\n        # Calculate market impact penalty (quadratic function)\n        market_impact_penalty = 0.0001 * (row['TRADE_VOLUME'] ** 2)\n\n        # Calculate total reward\n        reward = -execution_price_deviation - execution_speed_penalty -  commission_cost- market_impact_penalty\n        rewards.append(reward)\n\n    return rewards\n\nrewards = reward_function(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:29:20.235122Z","iopub.execute_input":"2024-06-09T20:29:20.235546Z","iopub.status.idle":"2024-06-09T20:29:45.257041Z","shell.execute_reply.started":"2024-06-09T20:29:20.235513Z","shell.execute_reply":"2024-06-09T20:29:45.256252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Normalizing the data\ndef normalize_data(df):\n    df_normalized = df.copy()\n    for column in df.columns:\n        if df[column].dtype == np.float64 or df[column].dtype == np.int64:\n            df_normalized[column] = (df[column] - df[column].mean()) / df[column].std()\n    return df_normalized\n\ndata = normalize_data(data)\ndata.drop(columns = ['DATE', 'ACTION','ORDERNO','TRADE_TIME'], inplace = True)\n# Create a dictionary to map string values to numeric values\nbuysell_map = {'B': 0, 'S': 1}\n\n# Map the string values to numeric values\ndata['BUYSELL'] = data['BUYSELL'].map(buysell_map)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:29:45.259209Z","iopub.execute_input":"2024-06-09T20:29:45.259608Z","iopub.status.idle":"2024-06-09T20:29:45.432133Z","shell.execute_reply.started":"2024-06-09T20:29:45.259575Z","shell.execute_reply":"2024-06-09T20:29:45.431274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns = 'TRADEPRICE', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:29:45.433754Z","iopub.execute_input":"2024-06-09T20:29:45.434340Z","iopub.status.idle":"2024-06-09T20:29:45.457909Z","shell.execute_reply.started":"2024-06-09T20:29:45.434301Z","shell.execute_reply":"2024-06-09T20:29:45.457074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Ensure no missing values remain in the dataset\ndata.fillna(0, inplace=True)\n\n# Define state representation\ndef get_state(df, step):\n    state = df.iloc[step].values.astype(np.float32)\n    return state\n\n# Define the Trading Environment\nclass TradingEnvironment:\n    def __init__(self, data):\n        self.data = data\n        self.current_step = 0\n        self.balance = 100000  # initial balance\n        self.shares_held = 0\n        self.initial_balance = 100000\n        self.done = False\n\n    def reset(self):\n        self.current_step = 0\n        self.balance = 100000\n        self.shares_held = 0\n        self.done = False\n        return get_state(self.data, self.current_step)\n\n    def step(self, action):\n        current_data = self.data.iloc[self.current_step]\n        current_price = current_data['PRICE']\n        bid_price = current_data['BID_PRICE']\n        ask_price = current_data['ASK_PRICE']\n        bid_ask_spread = current_data['BID_ASK_SPREAD']\n        market_direction = current_data['BUYSELL']\n\n        # Execute action\n        if action == 0:  # Buy\n            self.shares_held += 1\n            self.balance -= ask_price\n        elif action == 1:  # Sell\n            self.shares_held -= 1\n            self.balance += bid_price\n\n        # Calculate reward\n        total_value = self.balance + self.shares_held * current_price\n        pnl = total_value - self.initial_balance\n        transaction_costs = abs(self.shares_held) * bid_ask_spread\n        slippage = abs(current_price - (bid_price if action == 0 else ask_price))\n        market_impact = abs(self.shares_held) * current_price\n        market_direction_reward = 10 if (market_direction == 0 and action == 0) or (market_direction == 1 and action == 1) else -10 if (market_direction == 0 and action == 1) or (market_direction == 1 and action == 0) else 0\n\n        reward = pnl - transaction_costs - slippage - market_impact + market_direction_reward\n\n        # Move to the next step\n        self.current_step += 1\n        if self.current_step >= len(self.data) - 1:\n            self.done = True\n\n        return get_state(self.data, self.current_step), reward, self.done\n\n    def render(self):\n        pass\n\nclass QNetwork(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(QNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=10000)\n        self.gamma = 0.99  # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.batch_size = 40\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = QNetwork(state_size, action_size).to(self.device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        self.criterion = nn.MSELoss()\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        act_values = self.model(state)\n        return torch.argmax(act_values, dim=1).item()\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        minibatch = random.sample(self.memory, self.batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n            target = reward\n            if not done:\n                target = (reward + self.gamma * torch.max(self.model(next_state)).item())\n            target_f = self.model(state).detach().clone()\n            target_f[0][action] = target\n            self.optimizer.zero_grad()\n            loss = self.criterion(self.model(state), target_f)\n            loss.backward()\n            self.optimizer.step()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def load(self, name):\n        self.model.load_state_dict(torch.load(name))\n\n    def save(self, name):\n        torch.save(self.model.state_dict(), name)\n\n# Initialize environment and agent\nenv = TradingEnvironment(data)\nstate_size = env.data.shape[1]\naction_size = 3  # Buy, Sell, Hold\nagent = DQNAgent(state_size, action_size)\n\n# Training the agent\nepisodes = 100\nrewards = []\n\n# Initialize the plot\nplt.ion()\nfig, ax = plt.subplots()\nline, = ax.plot(rewards)\nplt.xlabel('Episode')\nplt.ylabel('Total Rewards')\nplt.title('Training Rewards over Episodes')\n\nfor e in tqdm(range(episodes)):\n    state = env.reset()\n    total_rewards = 0\n\n    while True:\n        action = agent.act(state)\n        next_state, reward, done = env.step(action)\n        reward = reward if not done else -10\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        total_rewards += reward\n        if done:\n            rewards.append(total_rewards)\n            print(f\"Episode {e+1}/{episodes} - Total reward: {total_rewards}, Epsilon: {agent.epsilon}\")\n            break\n        agent.replay()\n    \n    # Update the plot every 10 episodes\n    if (e + 1) % 10 == 0:\n        line.set_ydata(rewards)\n        line.set_xdata(range(len(rewards)))\n        ax.relim()\n        ax.autoscale_view()\n        plt.draw()\n        plt.pause(0.001)\n\n# Evaluation Metrics\ndef evaluate_agent(agent, env, episodes=10):\n    total_rewards = []\n    for e in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        while True:\n            action = np.argmax(agent.model(torch.FloatTensor(state).unsqueeze(0)).detach().numpy())\n            next_state, reward, done = env.step(action)\n            state = next_state\n            total_reward += reward\n            if done:\n                total_rewards.append(total_reward)\n                break\n\n    avg_reward = np.mean(total_rewards)\n    return avg_reward\n\naverage_reward = evaluate_agent(agent, env)\nprint(f\"Average reward over {episodes} episodes: {average_reward}\")\n\n# Plotting the final rewards\nplt.ioff()\nplt.plot(rewards)\nplt.xlabel('Episode')\nplt.ylabel('Total Rewards')\nplt.title('Training Rewards over Episodes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:34:39.070929Z","iopub.execute_input":"2024-06-09T20:34:39.071358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the final rewards\nplt.ioff()\nplt.plot(rewards)\nplt.xlabel('Episode')\nplt.ylabel('Total Rewards')\nplt.title('Training Rewards over Episodes')\nplt.show()\n\n# TWAP and VWAP Implementation for Comparison\ndef TWAP(data):\n    shares_to_trade = 100  # Total shares to trade\n    steps = len(data)\n    shares_per_step = shares_to_trade // steps\n    total_value = 0\n    for i in range(steps):\n        price = data.iloc[i]['PRICE']\n        total_value += shares_per_step * price\n    return total_value / shares_to_trade\n\ndef VWAP(data):\n    total_value = 0\n    total_volume = 0\n    for i in range(len(data)):\n        price = data.iloc[i]['PRICE']\n        volume = data.iloc[i]['VOLUME']\n        total_value += price * volume\n        total_volume += volume\n    return total_value / total_volume\n\ntwap_result = TWAP(data)\nvwap_result = VWAP(data)\n\nprint(f\"TWAP Result: {twap_result}\")\nprint(f\"VWAP Result: {vwap_result}\")\n\n# Heatmap for Strategy Visualization\naction_matrix = np.zeros((episodes, len(data)))\nfor i in range(episodes):\n    for j, action in enumerate(actions[i]):\n        action_matrix[i, j] = action\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(action_matrix, cmap=\"YlGnBu\", cbar=True)\nplt.xlabel('Time Step')\nplt.ylabel('Episode')\nplt.title('Heatmap of Agent Actions Over Episodes')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}