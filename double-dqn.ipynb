{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8648587,"sourceType":"datasetVersion","datasetId":5180305}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport gym\nimport pandas as pd \nimport seaborn as sns\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm  # Progress bar\nfrom sklearn.preprocessing import StandardScaler\nimport torch.optim as optim\nfrom collections import deque\nfrom sklearn.preprocessing import StandardScaler\nimport random\n# Normalize features\nfrom sklearn.preprocessing import StandardScaler\nfrom collections import deque\nimport collections\nfrom keras import backend as K\nimport tensorflow as tf\nfrom gym import spaces\nimport matplotlib.pyplot as plt\nK.clear_session()\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T20:51:54.324593Z","iopub.execute_input":"2024-06-09T20:51:54.324989Z","iopub.status.idle":"2024-06-09T20:52:16.796281Z","shell.execute_reply.started":"2024-06-09T20:51:54.324956Z","shell.execute_reply":"2024-06-09T20:52:16.794814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/eqqerff/preprocessed.csv')\ndata.drop(columns= ['Unnamed: 0','index'], inplace = True)\n# preprocessing\ndata['DATETIME'] = pd.to_datetime(data['DATETIME'])\ndata['TIME'] = data['TIME'].astype(str).str.rstrip('000')\ndata['hour'] = data['DATETIME'].dt.hour\ndata['minute'] = data['DATETIME'].dt.minute\ndata['day_of_week'] = data['DATETIME'].dt.day_name()\ndata['second'] =  data['DATETIME'].dt.second\nday_map = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\ndata['day_of_week'] = data['day_of_week'].map(day_map)\ndata.drop(columns= ['DATETIME','TIME'], inplace = True) \ndata.drop(columns= ['TRADE_PRICE','TRADENO'], inplace = True)\ndata= data.dropna()\ndata","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:52:16.798798Z","iopub.execute_input":"2024-06-09T20:52:16.799519Z","iopub.status.idle":"2024-06-09T20:52:21.537037Z","shell.execute_reply.started":"2024-06-09T20:52:16.799481Z","shell.execute_reply":"2024-06-09T20:52:21.535694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reward_function(df):\n    # Calculate best bid-ask spread for each date\n    best_spread_per_date = df.groupby('DATE').apply(lambda x: x['BID_PRICE'].max() - x['ASK_PRICE'].min())\n\n    rewards = []\n    for index, row in df.iterrows():\n        # Calculate execution price deviation from best bid-ask spread\n        execution_price_deviation = row['PRICE'] - best_spread_per_date[row['DATE']]\n\n        # Calculate execution speed penalty\n        execution_speed_penalty = 100 / 1000  # converting milliseconds to seconds\n\n        # Calculate commission cost\n        commission_cost = 0.01 * row['TRADE_VOLUME']\n        # Calculate market impact penalty (quadratic function)\n        market_impact_penalty = 0.0001 * (row['TRADE_VOLUME'] ** 2)\n\n        # Calculate total reward\n        reward = -execution_price_deviation - execution_speed_penalty -  commission_cost- market_impact_penalty\n        rewards.append(reward)\n\n    return rewards\n\nrewards = reward_function(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:52:21.542813Z","iopub.execute_input":"2024-06-09T20:52:21.543268Z","iopub.status.idle":"2024-06-09T20:52:51.809360Z","shell.execute_reply.started":"2024-06-09T20:52:21.543224Z","shell.execute_reply":"2024-06-09T20:52:51.808060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Normalizing the data\ndef normalize_data(df):\n    df_normalized = df.copy()\n    for column in df.columns:\n        if df[column].dtype == np.float64 or df[column].dtype == np.int64:\n            df_normalized[column] = (df[column] - df[column].mean()) / df[column].std()\n    return df_normalized\n\ndata = normalize_data(data)\ndata.drop(columns = ['DATE', 'ACTION','ORDERNO','TRADE_TIME'], inplace = True)\n# Create a dictionary to map string values to numeric values\nbuysell_map = {'B': 0, 'S': 1}\n\n# Map the string values to numeric values\ndata['BUYSELL'] = data['BUYSELL'].map(buysell_map)\ndata.drop(columns = 'TRADEPRICE', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:52:51.812220Z","iopub.execute_input":"2024-06-09T20:52:51.812708Z","iopub.status.idle":"2024-06-09T20:52:52.062037Z","shell.execute_reply.started":"2024-06-09T20:52:51.812656Z","shell.execute_reply":"2024-06-09T20:52:52.060639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:01:15.034777Z","iopub.execute_input":"2024-06-09T21:01:15.035264Z","iopub.status.idle":"2024-06-09T21:01:15.044266Z","shell.execute_reply.started":"2024-06-09T21:01:15.035227Z","shell.execute_reply":"2024-06-09T21:01:15.042946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure no missing values remain in the dataset\ndata.fillna(0, inplace=True)\n\n# Normalize data function using StandardScaler\ndef normalize_data(df):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n    return pd.DataFrame(scaled_data, columns=df.columns)\n\n# Neural Network architecture\nclass DQN(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, output_dim)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Experience Replay\nclass ReplayMemory:\n    def __init__(self, capacity):\n        self.memory = deque(maxlen=capacity)\n        \n    def push(self, transition):\n        self.memory.append(transition)\n        \n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n    \n    def __len__(self):\n        return len(self.memory)\n\n# Double DQN Agent\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, gamma=0.99, lr=0.001, replay_memory_size=10000, batch_size=32, target_update=10):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.epsilon = epsilon\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.gamma = gamma\n        self.lr = lr\n        self.batch_size = batch_size\n        self.target_update = target_update\n        self.memory = ReplayMemory(replay_memory_size)\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n        self.target_net = DQN(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        \n        self.update_target_network()\n\n    def update_target_network(self):\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n        \n    def select_action(self, state):\n        if np.random.rand() <= self.epsilon:\n            return np.random.randint(0, self.action_dim)\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            return self.policy_net(state).argmax().item()\n        \n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        \n        minibatch = self.memory.sample(self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*minibatch)\n        \n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device).unsqueeze(1)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n        \n        current_q_values = self.policy_net(states).gather(1, actions)\n        next_actions = self.policy_net(next_states).argmax(dim=1, keepdim=True)\n        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n        expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n        \n        loss = self.criterion(current_q_values, expected_q_values.unsqueeze(1))\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n            \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.push((state, action, reward, next_state, done))\n\n# Environment for Optimal Execution\nclass OptimalExecutionEnv:\n    def __init__(self, data):\n        self.data = data\n        self.n = len(data)\n        self.current_step = 0\n        \n    def reset(self):\n        self.current_step = 0\n        return self.data.iloc[self.current_step].values\n        \n    def step(self, action):\n        state = self.data.iloc[self.current_step].values\n        next_state = self.data.iloc[self.current_step + 1].values if self.current_step < self.n - 1 else state\n        reward = self.calculate_reward(self.data.iloc[self.current_step], action)\n        done = self.current_step == self.n - 1\n        self.current_step += 1\n        return next_state, reward, done\n        \n    def calculate_reward(self, row, action):\n        execution_price_deviation = row['PRICE'] - row['BID_ASK_SPREAD']\n        execution_speed_penalty = 100 / 1000\n        commission_cost = 0.01 * row['VOLUME']\n        market_impact_penalty = 0.0001 * (row['VOLUME'] ** 2)\n        reward = -execution_price_deviation - execution_speed_penalty - commission_cost - market_impact_penalty\n        return reward\n\n# Main training loop\ndef train(agent, env, episodes):\n    for e in tqdm(range(episodes), desc=\"Training Episodes\"):\n        state = env.reset()\n        total_rewards = 0\n        while True:\n            action = agent.select_action(state)\n            next_state, reward, done = env.step(action)\n            total_rewards += reward\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                print(f\"Episode {e+1}/{episodes} - Total reward: {total_rewards}, Epsilon: {agent.epsilon}\")\n                break\n            agent.replay()\n        if e % agent.target_update == 0:\n            agent.update_target_network()\n\n# Data Preprocessing\ndata = normalize_data(data)\nstate_dim = data.shape[1]\naction_dim = 2  # Buy or Sell\n\n# Initialize environment and agent\nenv = OptimalExecutionEnv(data)\nagent = DQNAgent(state_dim=state_dim, action_dim=action_dim)\n\n# Train the agent\ntrain(agent, env, episodes=100)\n\n# Evaluate the agent\ndef evaluate_agent(agent, env, episodes=10):\n    total_rewards = []\n    for _ in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        while True:\n            action = agent.select_action(state)\n            next_state, reward, done = env.step(action)\n            total_reward += reward\n            state = next_state\n            if done:\n                total_rewards.append(total_reward)\n                break\n    average_reward = np.mean(total_rewards)\n    print(f\"Average Reward over {episodes} episodes: {average_reward}\")\n\nevaluate_agent(agent, env, episodes=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:07:09.573425Z","iopub.execute_input":"2024-06-09T21:07:09.574377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate statistics\ndef calculate_statistics(rewards):\n    avg_reward = np.mean(rewards)\n    max_reward = np.max(rewards)\n    min_reward = np.min(rewards)\n    std_reward = np.std(rewards)\n    return avg_reward, max_reward, min_reward, std_reward\n\n# Plot training performance\ndef plot_training_performance(rewards):\n    plt.figure(figsize=(10, 6))\n    plt.plot(rewards)\n    plt.title(\"Training Performance\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.grid(True)\n    plt.show()\n\n# Plot Optimal Actions for All Possible States\ndef plot_optimal_actions(agent, env):\n    states = np.array([[t, i, p] for t in range(env.n) for i in range(agent.state_dim) for p in range(agent.state_dim)])\n    actions = [agent.select_action(state) for state in states]\n    actions = np.array(actions).reshape((env.n, agent.state_dim, agent.state_dim))\n    \n    plt.figure(figsize=(15, 10))\n    for i in range(agent.state_dim):\n        for j in range(agent.state_dim):\n            plt.subplot(agent.state_dim, agent.state_dim, i * agent.state_dim + j + 1)\n            sns.heatmap(actions[:, i, j], cmap=\"YlGnBu\", cbar=False, xticklabels=False, yticklabels=False)\n            plt.title(f\"Price: {i}, Inventory: {j}\")\n            plt.xlabel(\"Time\")\n            plt.ylabel(\"Inventory Remaining\")\n    plt.tight_layout()\n    plt.show()\n\n# Train the agent\nrewards = train(agent, env, episodes=100)\n\n# Calculate statistics\navg_reward, max_reward, min_reward, std_reward = calculate_statistics(rewards)\nprint(f\"Average Reward: {avg_reward}\")\nprint(f\"Max Reward: {max_reward}\")\nprint(f\"Min Reward: {min_reward}\")\nprint(f\"Standard Deviation of Reward: {std_reward}\")\n\n# Plot training performance\nplot_training_performance(rewards)\n\n# Plot Optimal Actions for All Possible States\nplot_optimal_actions(agent, env)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}